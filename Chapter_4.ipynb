{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rnarkk/jupyter/blob/main/Bayesian-Methods-for-Hackers/Chapter_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2VgkczEJZhY"
      },
      "source": [
        "# Probabilistic Programming and Bayesian Methods for Hackers Chapter 4\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_TFP.ipynb\"><img height=\"32px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_TFP.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Original content ([this Jupyter notebook](https://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC2.ipynb)) created by Cam Davidson-Pilon ([`@Cmrn_DP`](https://twitter.com/Cmrn_DP))\n",
        "\n",
        "Welcome to Bayesian Methods for Hackers. The full Github repository is available at [github/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers). The other chapters can be found on the project's [homepage](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/). We hope you enjoy the book, and we encourage any contributions!\n",
        "\n",
        "---------\n",
        "### Table of Contents\n",
        "- The greatest theorem never told\n",
        "  - The Law of Large Numbers\n",
        "  - Intuition\n",
        "  - How do we compute $Var(Z)$ though?\n",
        "  - Expected values and probabilities\n",
        "  - What does this all have to do with Bayesian statistics?\n",
        "  - The Disorder of Small Numbers\n",
        "  - Example: Aggregated geographic data\n",
        "  - Example: Kaggle's U.S. Census Return Rate Challenge\n",
        "  - Example: How to order Reddit submissions\n",
        "    - Setting up the Praw Reddit API\n",
        "    - Register your Application on Reddit\n",
        "      - Reddit API Setup\n",
        "    - Sorting!\n",
        "    - But this is too slow for real-time!\n",
        "  - Extension to Starred rating systems\n",
        "  - Example: Counting Github stars\n",
        "  - Conclusion\n",
        "  - Appendix\n",
        "    - Exercises\n",
        "    - Kicker Careers Ranked by Make Percentage\n",
        "    - Average Household Income by Programming Language\n",
        "  - References\n",
        "\n",
        "______\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QElsebmh-WTn",
        "outputId": "9cfd8911-6b53-46c0-d359-2c3b791f0529"
      },
      "source": [
        "!pip3 install -U pip\n",
        "!pip3 uninstall -y tensorflow tensorflow-probability\n",
        "!pip3 install -U jax tfp-nightly[jax]\n",
        "\n",
        "!pip3 install -q praw wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/ca/f0d790b6e18b3a6f3bd5e80c2ee4edbb5807286c21cdd0862ca933f751dd/pip-21.1.3-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 26.4MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 32.7MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 27.5MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 13.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 15.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 14.9MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 13.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133kB 14.7MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 14.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153kB 14.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163kB 14.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174kB 14.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184kB 14.7MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 14.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 14.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 14.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225kB 14.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 245kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 266kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 276kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 337kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 348kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 358kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 368kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 378kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 389kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 399kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 409kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 419kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 450kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 460kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 471kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 491kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 501kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 512kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 522kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 532kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 542kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 552kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 563kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 573kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 593kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 604kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 614kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 634kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 645kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 655kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 665kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 675kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 686kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 696kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 706kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 716kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 727kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 737kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 747kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 757kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 768kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 778kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 788kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 798kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 808kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 819kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 829kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 839kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 849kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 860kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 870kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 880kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 890kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 901kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 911kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 921kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 931kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 942kB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 952kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 962kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 972kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 983kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 993kB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 14.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6MB 14.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.1.3\n",
            "Found existing installation: tensorflow 2.5.0\n",
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "Found existing installation: tensorflow-probability 0.12.1\n",
            "Uninstalling tensorflow-probability-0.12.1:\n",
            "  Successfully uninstalled tensorflow-probability-0.12.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.2.13)\n",
            "Collecting jax\n",
            "  Downloading jax-0.2.16.tar.gz (680 kB)\n",
            "\u001b[K     |████████████████████████████████| 680 kB 13.7 MB/s \n",
            "\u001b[?25hCollecting tfp-nightly[jax]\n",
            "  Downloading tfp_nightly-0.14.0.dev20210630-py2.py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from jax) (1.19.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (0.12.0)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax) (1.15.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (0.1.6)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (0.4.0)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (0.1.66+cuda110)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from jaxlib->tfp-nightly[jax]) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib->tfp-nightly[jax]) (1.4.1)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.2.16-py3-none-any.whl size=784392 sha256=ab0f6793d52b43ce2e8dc2fe7777ac143e77729120725ae6b9d8d11397e6d199\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/23/9e/4da95cc1faef02199df99d4928b8e870a7c3de039403dcbc6c\n",
            "Successfully built jax\n",
            "Installing collected packages: tfp-nightly, jax\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.2.13\n",
            "    Uninstalling jax-0.2.13:\n",
            "      Successfully uninstalled jax-0.2.13\n",
            "Successfully installed jax-0.2.16 tfp-nightly-0.14.0.dev20210630\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jtWqJhSJZhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "983d7412-fdf9-4232-ba4d-5d2b707ba2ce"
      },
      "source": [
        "\"\"\"\n",
        "The book uses a custom matplotlibrc file, which provides the unique styles for\n",
        "matplotlib plots. If executing this book, and you wish to use the book's\n",
        "styling, provided are two options:\n",
        "    1. Overwrite your own matplotlibrc file with the rc-file provided in the\n",
        "       book's styles/ dir. See http://matplotlib.org/users/customizing.html\n",
        "    2. Also in the styles is  bmh_matplotlibrc.json file. This can be used to\n",
        "       update the styles in only this notebook. Try running the following code:\n",
        "\n",
        "        import json\n",
        "        s = json.load(open(\"../styles/bmh_matplotlibrc.json\"))\n",
        "        matplotlib.rcParams.update(s)\n",
        "\"\"\"\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "import jax.numpy as np\n",
        "from jax.numpy import (\n",
        "    int32 as i32,\n",
        "    float32 as f32)\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "tfd = tfp.distributions\n",
        "Binomial, Exponential, Normal, Poisson, Uniform = \\\n",
        "    tfd.Binomial, tfd.Exponential, tfd.Normal, tfd.Poisson, tfd.Uniform\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import wget\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set_context('notebook')\n",
        "\n",
        "TFColor = [\n",
        "    '#F15854',\n",
        "    '#5DA5DA',\n",
        "    '#FAA43A',\n",
        "    '#60BD68',\n",
        "    '#F17CB0',\n",
        "    '#B2912F',\n",
        "    '#B276B2',\n",
        "    '#DECF3F',\n",
        "    '#4D4D4D']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "Uninstalling tensorflow-probability-0.12.1:\n",
            "  Successfully uninstalled tensorflow-probability-0.12.1\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/a6/9bf3b86c587d253fb90a576f6ade5983810f707c77a2cd27457b2add7c68/tf_nightly-2.7.0.dev20210707-cp37-cp37m-manylinux2010_x86_64.whl (458.7MB)\n",
            "\u001b[K     |████████████████████████████████| 458.7MB 34kB/s \n",
            "\u001b[?25hCollecting tfp-nightly[jax]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/83/5a8fd4bfdb4a34f6d8c0bf351b4d09c0a12c920ca3d0fedd54e140885a57/tfp_nightly-0.14.0.dev20210630-py2.py3-none-any.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 21.5MB/s \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ca/2bbd8f95d03fac3e8f07563e0e3d9d8ec8e573f5ec61221536f4e854801d/grpcio-1.38.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\n",
            "Collecting tb-nightly~=2.6.0.a\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/d0/ecbf9a34fd6a8ffb9ae31cb16b261c897526210d6b0c0eaeeb7dcada1100/tb_nightly-2.6.0a20210706-py3-none-any.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 27.5MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly~=2.6.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/8e/68eadcabf0115a75d47effafb18fb680fd152f0cf0b10a47df4a6ee2b7d6/tf_estimator_nightly-2.6.0.dev2021062501-py2.py3-none-any.whl (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 45.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\n",
            "Collecting keras-nightly~=2.6.0.dev\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/6c/9d1cc27adcc563fb470e1cefc3a6645bb1c78eb7460414ce57a1275d9773/keras_nightly-2.6.0.dev2021062500-py2.py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 27.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\n",
            "Collecting libclang~=12.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/2d/7b0f7f5519669f11e66028fa227d4bcda4d77411d52d26c661676db82338/libclang-12.0.0-py2.py3-none-manylinux1_x86_64.whl (13.4MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4MB 240kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: dm-tree in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (0.1.6)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: jaxlib; extra == \"jax\" in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (0.1.66+cuda110)\n",
            "Requirement already satisfied, skipping upgrade: jax; extra == \"jax\" in /usr/local/lib/python3.7/dist-packages (from tfp-nightly[jax]) (0.2.13)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.6.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tf-nightly) (1.5.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib; extra == \"jax\"->tfp-nightly[jax]) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (4.5.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2021.5.30)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (3.1.1)\n",
            "Installing collected packages: grpcio, tb-nightly, tf-estimator-nightly, keras-nightly, libclang, tf-nightly, tfp-nightly\n",
            "  Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "  Found existing installation: keras-nightly 2.5.0.dev2021032900\n",
            "    Uninstalling keras-nightly-2.5.0.dev2021032900:\n",
            "      Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n",
            "Successfully installed grpcio-1.38.1 keras-nightly-2.6.0.dev2021062500 libclang-12.0.0 tb-nightly-2.6.0a20210706 tf-estimator-nightly-2.6.0.dev2021062501 tf-nightly-2.7.0.dev20210707 tfp-nightly-0.14.0.dev20210630\n",
            "\u001b[K     |████████████████████████████████| 174kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e53edfae73d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# import tensorflow_probability as tfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstrates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mtfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# `python/__init__.py` as necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubstrates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/substrates/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_loader\u001b[0m  \u001b[0;31m# pylint: disable=g-direct-tensorflow-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;31m# Non-lazy load of packages that register with tensorflow or keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpkg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_maybe_nonlazy_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forces loading the package from its lazy loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbijectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/experimental/bijectors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"TensorFlow Probability experimental bijectors package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldj_ratio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_log_det_jacobian_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldj_ratio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minverse_log_det_jacobian_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_bijectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_distribution_bijector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneralized_pareto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneralizedPareto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgev_cdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGeneralizedExtremeValueCDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlowDefaultExitNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbijectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlowDefaultNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/glow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtfk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtfkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_loss_scale_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/_v2/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '__version__' from 'keras' (/usr/local/lib/python3.7/dist-packages/keras/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AADx_YZYRbo1"
      },
      "source": [
        "## The greatest theorem never told\n",
        "\n",
        "\n",
        "This chapter focuses on an idea that is always bouncing around our minds, but is rarely made explicit outside books devoted to statistics. In fact, we've been using this simple idea in every example thus far. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8r47e-LJZhf"
      },
      "source": [
        "### The Law of Large Numbers\n",
        "\n",
        "Let $Z_i$ be $N$ independent samples from some probability distribution. According to *the Law of Large numbers*,  so long as the expected value $E[Z]$ is finite, the following holds,\n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N Z_i \\rightarrow E[ Z ],  \\;\\;\\; N \\rightarrow \\infty.$$\n",
        "\n",
        "In words:\n",
        "\n",
        ">   The average of a sequence of random variables from the same distribution converges to the expected value of that distribution.\n",
        "\n",
        "This may seem like a boring result, but it will be the most useful tool you use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0J76mQaJZhg"
      },
      "source": [
        "### Intuition \n",
        "\n",
        "If the above Law is somewhat surprising,  it can be made more clear by examining a simple example. \n",
        "\n",
        "Consider a random variable $Z$ that can take only two values, $c_1$ and $c_2$. Suppose we have a large number of samples of $Z$, denoting a specific sample $Z_i$. The Law says that we can approximate the expected value of $Z$ by averaging over all samples. Consider the average:\n",
        "\n",
        "\n",
        "$$ \\frac{1}{N} \\sum_{i=1}^N Z_i $$\n",
        "\n",
        "\n",
        "By construction, $Z_i$ can only take on $c_1$ or $c_2$, hence we can partition the sum over these two values:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{1}{N} \\sum_{i=1}^N Z_i & =\\frac{1}{N} \\big(  \\sum_{ Z_i = c_1}c_1 + \\sum_{Z_i=c_2}c_2 \\big) \\\\\n",
        "& = c_1 \\sum_{ Z_i = c_1}\\frac{1}{N} + c_2 \\sum_{ Z_i = c_2}\\frac{1}{N} \\\\\n",
        "& = c_1 \\times \\text{ (approximate frequency of $c_1$) } \\\\\n",
        "& \\;\\;\\;\\;\\;\\;\\;\\;\\; + c_2 \\times \\text{ (approximate frequency of $c_2$) } \\\\\n",
        "& \\approx c_1 \\times P(Z = c_1) + c_2 \\times P(Z = c_2 ) \\\\\n",
        "& = E[Z]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Equality holds in the limit, but we can get closer and closer by using more and more samples in the average. This Law holds for almost *any distribution*, minus some important cases we will encounter later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subV2qHrrUsj"
      },
      "source": [
        "### Example\n",
        "____\n",
        "\n",
        "\n",
        "Below is a diagram of the Law of Large numbers in action for three different sequences of Poisson random variables. \n",
        "\n",
        " We sample `sample_size = 100000` Poisson random variables with parameter $\\lambda = 4.5$. (Recall the expected value of a Poisson random variable is equal to its parameter.) We calculate the average for the first $n$ samples, for $n=1$ to `sample_size`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xOv_zU34Tci"
      },
      "source": [
        "sample_size = 100000\n",
        "expected_value = lambda_val = 4.5\n",
        "N_samples = np.arange(start = 1, stop = sample_size, step = 100)\n",
        "\n",
        "plt.figure(figsize(12.5, 4))\n",
        "for k in range(3):\n",
        "    samples = Poisson(rate = lambda_val).sample(sample_shape = sample_size)\n",
        "    partial_average = [np.mean(samples[:i]) for i in N_samples]        \n",
        "    plt.plot(N_samples, partial_average, lw=1.5, label=f\"Average of $n$ samples; seq. {k}\")\n",
        "\n",
        "plt.plot(N_samples, expected_value * np.ones_like(partial_average), \n",
        "         ls='--', label=\"True expected value\", c='k')\n",
        "\n",
        "plt.ylim(4.35, 4.65) \n",
        "plt.title(\"Convergence of the average of \\n random variables to its expected value\")\n",
        "plt.ylabel(\"Average of $n$ samples\")\n",
        "plt.xlabel(\"Number of samples, $n$\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbUnsQXuJZhn"
      },
      "source": [
        "Looking at the above plot, it is clear that when the sample size is small, there is greater variation in the average (compare how *jagged and jumpy* the average is initially, then *smooths* out). All three paths *approach* the value 4.5, but just flirt with it as $N$ gets large. Mathematicians and statistician have another name for *flirting*: convergence. \n",
        "\n",
        "Another very relevant question we can ask is *how quickly am I converging to the expected value?* Let's plot something new. For a specific $N$, let's do the above trials thousands of times and compute how far away we are from the true expected value, on average. But wait &mdash; *compute on average*? This is simply the law of large numbers again! For example, we are interested in, for a specific $N$, the quantity:\n",
        "\n",
        "$$D(N) = \\sqrt{E\\left[ \\left( \\frac{1}{N}\\sum_{i=1}^N Z_i  - 4.5 \\right)^2 \\right] }$$\n",
        "\n",
        "The above formulae is interpretable as a distance away from the true value (on average), for some $N$. (We take the square root so the dimensions of the above quantity and our random variables are the same). As the above is an expected value, it can be approximated using the law of large numbers: instead of averaging $Z_i$, we calculate the following multiple times and average them:\n",
        "\n",
        "$$ Y_k = \\left( \\frac{1}{N}\\sum_{i=1}^NZ_i  - 4.5 \\right)^2 $$\n",
        "\n",
        "By computing the above many, $N_y$, times (remember, it is random), and averaging them:\n",
        "\n",
        "$$ \\frac{1}{N_Y} \\sum_{k=1}^{N_Y} Y_k \\rightarrow E[Y_k] = E\\left[\\left( \\frac{1}{N}\\sum_{i=1}^N Z_i - 4.5 \\right)^2 \\right]$$\n",
        "\n",
        "Finally, taking the square root:\n",
        "\n",
        "$$ \\sqrt{\\frac{1}{N_Y} \\sum_{k=1}^{N_Y} Y_k} \\approx D(N) $$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbPdvgcKKGxY"
      },
      "source": [
        "N_Y = tf.constant(250)  # Use this many to approximate D(N)\n",
        "N_array = np.arange(1000., 50000., 2500)  # Use this many samples in the approx. to the variance.\n",
        "lambda_val = tf.constant(4.5) \n",
        "expected_value = tf.constant(4.5)  # For X ~ Poi(lambda) , E[X] = lambda\n",
        "\n",
        "\n",
        "def D_N(n):\n",
        "    \"\"\"This function approx. D_n, the average variance of using n samples.\"\"\"\n",
        "    Z = Poisson(rate = lambda_val).sample(sample_shape = (int(n), int(N_Y)))\n",
        "    mean_Z = np.mean(Z, axis = 0)\n",
        "    return np.mean(np.sqrt(((mean_Z - expected_value) ** 2)))\n",
        "\n",
        "D_N_results = tf.convert_to_tensor([D_N(n) for n in N_array])\n",
        "\n",
        "plt.figure(figsize(12.5, 3))\n",
        "plt.xlabel(\"$N$\")\n",
        "plt.ylabel(\"Expected squared-distance\\nfrom true value\")\n",
        "plt.plot(N_array, D_N_results, lw=3, \n",
        "         label=\"Expected distance between\\nexpected value and\\naverage of $N$ random variables\")\n",
        "plt.plot(N_array, np.sqrt(expected_value) / np.sqrt(N_array), lw=2, ls='--', \n",
        "         label=r\"$\\frac{\\sqrt{\\lambda}}{\\sqrt{N}}$\" )\n",
        "plt.legend()\n",
        "plt.title(\"How 'fast' is the sample average converging?\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps0kCQIpJZhr"
      },
      "source": [
        "As expected, the expected distance between our sample average and the actual expected value shrinks as $N$ grows large. But also notice that the *rate* of convergence decreases, that is, we need only 10 000 additional samples to move from 0.020 to 0.015, a difference of 0.005, but *20 000* more samples to again decrease from 0.015  to 0.010, again only a 0.005 decrease.\n",
        "\n",
        "\n",
        "It turns out we can measure this rate of convergence. Above I have plotted a second line, the function $\\sqrt{\\lambda}/\\sqrt{N}$. This was not chosen arbitrarily. In most cases, given a sequence of random variable distributed like $Z$, the rate of convergence to $E[Z]$ of the Law of Large Numbers is \n",
        "\n",
        "$$ \\frac{ \\sqrt{ Var(Z) } }{\\sqrt{N} }$$\n",
        "\n",
        "This is useful to know: for a given large $N$, we know (on average) how far away we are from the estimate. On the other hand, in a Bayesian setting, this can seem like a useless result: Bayesian analysis is OK with uncertainty so what's the *statistical* point of adding extra precise digits? Though drawing samples can be so computationally cheap that having a *larger* $N$ is fine too. \n",
        "\n",
        "### How do we compute $Var(Z)$ though?\n",
        "\n",
        "The variance is simply another expected value that can be approximated! Consider the following, once we have the expected value (by using the Law of Large Numbers to estimate it, denote it $\\mu$), we can estimate the variance:\n",
        "\n",
        "$$ \\frac{1}{N}\\sum_{i=1}^N (Z_i - \\mu)^2 \\rightarrow E[(Z - \\mu)^2] = Var(Z) $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQBZaHpGH9R0"
      },
      "source": [
        "### Expected values and probabilities \n",
        "There is an even less explicit relationship between expected value and estimating probabilities. Define the *indicator function*\n",
        "\n",
        "$$\\mathbb{1}_A(x) = \n",
        "\\begin{cases} 1 &  x \\in A \\\\\\\\\n",
        "              0 &  else\n",
        "\\end{cases}\n",
        "$$\n",
        "Then, by the law of large numbers, if we have many samples $X_i$, we can estimate the probability of an event $A$, denoted $P(A)$, by:\n",
        "\n",
        "$$ \\frac{1}{N} \\sum_{i=1}^N \\mathbb{1}_A(X_i) \\rightarrow E[\\mathbb{1}_A(X)] =  P(A) $$\n",
        "\n",
        "Again, this is fairly obvious after a moments thought: the indicator function is only 1 if the event occurs, so we are summing only the times the event occurs and dividing by the total number of trials  (consider how we usually approximate probabilities using frequencies). For example, suppose we wish to estimate the probability that a $Z \\sim Exp(.5)$ is greater than 5, and we have many samples from a $Exp(.5)$ distribution. \n",
        "\n",
        "\n",
        "$$ P( Z > 5 ) =  \\frac{1}{N}\\sum_{i=1}^N \\mathbb{1}_{z > 5 }(Z_i) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtR6K5PFJZhr"
      },
      "source": [
        "N = 10000\n",
        "\n",
        "print(\"Probability Estimate:\", np.shape(np.where(Exponential(rate = .5).sample(sample_shape = (N,)) > 5))[1] / N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLQ3PC9VJZhu"
      },
      "source": [
        "### What does this all have to do with Bayesian statistics? \n",
        "\n",
        "\n",
        "*Point estimates*, to be introduced in the next chapter, in Bayesian inference are computed using expected values. In more analytical Bayesian inference, we would have been required to evaluate complicated expected values represented as multi-dimensional integrals. No longer. If we can sample from the posterior distribution directly, we simply need to evaluate averages. Much easier. If accuracy is a priority, plots like the ones above show how fast you are converging. And if further accuracy is  desired, just take more samples from the posterior. \n",
        "\n",
        "When is enough enough? When can you stop drawing samples from the posterior? That is the practitioners decision, and also dependent on the variance of the samples (recall from above a high variance means the average will converge slower). \n",
        "\n",
        "We also should understand when the Law of Large Numbers fails. As the name implies, and comparing the graphs above for small $N$, the Law is only true for large sample sizes. Without this, the asymptotic result is not reliable. Knowing in what situations the Law fails can give us *confidence in how unconfident we should be*. The next section deals with this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1VCON0qdlVM"
      },
      "source": [
        "### The Disorder of Small Numbers\n",
        "\n",
        "The Law of Large Numbers is only valid as $N$ gets *infinitely* large: never truly attainable.  While the law is a powerful tool, it is foolhardy to apply it liberally. Our next example illustrates this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6dhCexPJZhu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Example: Aggregated geographic data\n",
        "\n",
        "\n",
        "Often data comes in aggregated form. For instance, data may be grouped by state, county, or city level. Of course, the population numbers vary per geographic area. If the data is an average of some characteristic of each the geographic areas, we must be conscious of the Law of Large Numbers and how it can *fail* for areas with small populations.\n",
        "\n",
        "We will observe this on a toy dataset. Suppose there are five thousand counties in our dataset. Furthermore,  population number in each state are uniformly distributed between 100 and 1500. The way the population numbers are generated is irrelevant to the discussion, so we do not justify this. We are interested in measuring the average height of individuals per county. Unbeknownst to us, height does **not** vary across county, and each individual, regardless of the county he or she is currently living in, has the same distribution of what their height may be:\n",
        "\n",
        "$$ \\text{height} \\sim \\text{Normal}(\\text{loc}=150, \\text{scale}=15 ) $$\n",
        "\n",
        "We aggregate the individuals at the county level, so we only have data for the *average in the county*. What might our dataset look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zy3phgWaLKm"
      },
      "source": [
        "plt.figure(figsize(12.5, 4))\n",
        "\n",
        "std_height = 15.\n",
        "mean_height = 150.\n",
        "num_counties = 5000\n",
        "smallest_population = 100\n",
        "largest_population = 1500\n",
        "\n",
        "population = np.random.randint(smallest_population, largest_population, num_counties)\n",
        "\n",
        "# Our strategy to vectorise this problem will be to end-to-end concatenate the\n",
        "# number of draws we need. Then we'll loop over the pieces.\n",
        "d = Normal(loc = mean_height, scale =  1. / std_height)\n",
        "x = d.sample(np.sum(population))\n",
        "average_across_county_array = []\n",
        "seen = 0\n",
        "for p in population:\n",
        "    average_across_county_array.append(np.mean(x[seen:seen+p]))\n",
        "    seen += p\n",
        "average_across_county = tf.stack(average_across_county_array)\n",
        "# Locate the counties with the apparently most extreme average heights.\n",
        "i_min, i_max = np.argmin(average_across_county), np.argmax(average_across_county)\n",
        "\n",
        "# Plot population size vs. recorded average\n",
        "plt.scatter(population, average_across_county, alpha=.5, c=TFColor[6])\n",
        "plt.scatter([population[i_min], population[i_max]],\n",
        "            tf.stack([average_across_county[i_min], average_across_county[i_max]]),\n",
        "            s=60, marker='o', facecolors='none',\n",
        "            edgecolors=TFColor[0], linewidths=1.5, \n",
        "            label='Extreme heights')\n",
        "\n",
        "plt.xlim(smallest_population, largest_population)\n",
        "plt.title(\"Average height vs. County Population\")\n",
        "plt.xlabel('County Population')\n",
        "plt.ylabel('Average height in county')\n",
        "plt.plot([smallest_population, largest_population], [mean_height, mean_height], color='k',\n",
        "         label='True expected height', ls='--')\n",
        "plt.legend(scatterpoints=1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0M9eoo1JZhy"
      },
      "source": [
        "What do we observe? *Without accounting for population sizes* we run the risk of making an enormous inference error: if we ignored population size, we would say that the county with the shortest and tallest individuals have been correctly circled. But this inference is wrong for the following reason. These two counties do *not* necessarily have the most extreme heights. The error results from the calculated average of smaller populations not being a good reflection of the true expected value of the population (which in truth should be $\\mu =150$). The sample size/population size/$N$, whatever you wish to call it,  is simply too small to invoke the Law of Large Numbers effectively. \n",
        "\n",
        "We provide more damning evidence against this inference. Recall the population numbers were uniformly distributed over 100 to 1500. Our intuition should tell us that the counties with the most extreme population heights should also be uniformly spread over 100 to 1500, and certainly independent of the county's population. Not so. Below are the population sizes of the counties with the most extreme heights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcop0j8IJZhz"
      },
      "source": [
        "print(\"Population sizes of 10 'shortest' counties:\", population[np.argsort(average_across_county)[:10]])\n",
        "print(\"Population sizes of 10 'tallest' counties:\", population[np.argsort(-average_across_county)[:10]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObSCVpkXJZh2"
      },
      "source": [
        "Not at all uniform over 100 to 1500. This is an absolute failure of the Law of Large Numbers. \n",
        "\n",
        "### Example:  Kaggle's *U.S. Census Return Rate Challenge*\n",
        "\n",
        "Below is data from the 2010 US census, which partitions populations beyond counties to the level of block groups (which are aggregates of city blocks or equivalents). The dataset is from a Kaggle machine learning competition some colleagues and I participated in. The objective was to predict the census letter mail-back rate of a group block, measured between 0 and 100, using census variables (median income, number of females in the block-group, number of trailer parks, average number of children etc.). Below we plot the census mail-back rate versus block group population:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sae3sK9psnYi"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter4_TheGreatestTheoremNeverTold/data/census_data.csv'\n",
        "filename = wget.download(url)\n",
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0WIEfZ3JZh2"
      },
      "source": [
        "plt.figure(figsize(12.5, 6.5))\n",
        "data = np.genfromtxt('census_data.csv',\n",
        "                     skip_header = 1, \n",
        "                     delimiter = ',')\n",
        "plt.scatter(data[:, 1], data[:, 0], alpha=.5, c=TFColor[6])\n",
        "plt.title(\"Census mail-back rate vs population\")\n",
        "plt.ylabel('Mail-back rate')\n",
        "plt.xlabel('Population of block-group')\n",
        "plt.xlim(-100, 15e3)\n",
        "plt.ylim(-5, 105)\n",
        "\n",
        "i_min = np.argmin(data[:, 0])\n",
        "i_max = np.argmax(data[:, 0])\n",
        " \n",
        "plt.scatter([data[i_min, 1], data[i_max, 1]], \n",
        "            [data[i_min, 0], data[i_max, 0]],\n",
        "            s=60, marker='o', facecolors='none',\n",
        "            edgecolors=TFColor[0], linewidths=1.5, \n",
        "            label=\"Most extreme points\")\n",
        "\n",
        "plt.legend(scatterpoints=1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSU2zWC3JZh5"
      },
      "source": [
        "The above is a classic phenomenon in statistics. I say *classic* referring to the \"shape\" of the scatter plot above. It follows a classic triangular form, that tightens as we increase the sample size (as the Law of Large Numbers becomes more exact). \n",
        "\n",
        "I am perhaps overstressing the point and maybe I should have titled the book *\"You don't have big data problems!\"*, but here again is an example of the trouble with *small datasets*, not big ones. Simply, small datasets cannot be processed using the Law of Large Numbers. Compare with applying the Law without hassle to big datasets (ex. big data). I mentioned earlier that paradoxically big data prediction problems are solved by relatively simple algorithms. The paradox is partially resolved by understanding that the Law of Large Numbers creates solutions that are *stable*, i.e. adding or subtracting a few data points will not affect the solution much. On the other hand, adding or removing data points to a small dataset can create very different results. \n",
        "\n",
        "For further reading on the hidden dangers of the Law of Large Numbers, I would highly recommend the excellent manuscript [The Most Dangerous Equation](http://nsm.uh.edu/~dgraur/niv/TheMostDangerousEquation.pdf). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yULJJOOOJZh5"
      },
      "source": [
        "### Example: How to order Reddit submissions\n",
        "\n",
        "You may have disagreed with the original statement that the Law of Large numbers is known to everyone, but only implicitly in our subconscious decision making. Consider ratings on online products: how often do you trust an average 5-star rating if there is only 1 reviewer? 2 reviewers? 3 reviewers? We implicitly understand that with such few reviewers that the average rating is **not** a good reflection of the true value of the product.\n",
        "\n",
        "This has created flaws in how we sort items, and more generally, how we compare items. Many people have realized that sorting online search results by their rating, whether the objects be books, videos, or online comments, return poor results. Often the seemingly top videos or comments have perfect ratings only from a few enthusiastic fans, and truly more quality videos or comments are hidden in later pages with *falsely-substandard* ratings of around 4.8. How can we correct this?\n",
        "\n",
        "Consider the popular site Reddit (I purposefully did not link to the website as you would never come back). The site hosts links to stories or images, called submissions, for people to comment on. Redditors can vote up or down on each submission (called upvotes and downvotes). Reddit, by default, will sort submissions to a given subreddit by Hot, that is, the submissions that have the most upvotes recently.\n",
        "\n",
        "<img src=\"http://i.imgur.com/3v6bz9f.png\" />\n",
        "\n",
        "\n",
        "How would you determine which submissions are the best? There are a number of ways to achieve this:\n",
        "\n",
        "1. *Popularity*: A submission is considered good if it has many upvotes. A problem with this model is that a submission with hundreds of upvotes, but thousands of downvotes. While being very *popular*, the submission is likely more controversial than best.\n",
        "2. *Difference*: Using the *difference* of upvotes and downvotes. This solves the above problem, but fails when we consider the temporal nature of submission. Depending on when a submission is posted, the website may be experiencing high or low traffic. The difference method will bias the *Top* submissions to be the those made during high traffic periods, which have accumulated more upvotes than submissions that were not so graced, but are not necessarily the best.\n",
        "3. *Time adjusted*:  Consider using Difference divided by the age of the submission. This creates a *rate*, something like *difference per second*, or *per minute*. An immediate counter-example is, if we use per second, a 1 second old submission with 1 upvote would be better than a 100 second old submission with 99 upvotes. One can avoid this by only considering at least t second old submission. But what is a good t value? Does this mean no submission younger than t is good? We end up comparing unstable quantities with stable quantities (young vs. old submissions).\n",
        "3. *Ratio*: Rank submissions by the ratio of upvotes to total number of votes (upvotes plus downvotes). This solves the temporal issue, such that new submissions who score well can be considered Top just as likely as older submissions, provided they have many upvotes to total votes. The problem here is that a submission with a single upvote (ratio = 1.0) will beat a submission with 999 upvotes and 1 downvote (ratio = 0.999), but clearly the latter submission is *more likely* to be better.\n",
        "\n",
        "I used the phrase *more likely* for good reason. It is possible that the former submission, with a single upvote, is in fact a better submission than the later with 999 upvotes. The hesitation to agree with this is because we have not seen the other 999 potential votes the former submission might get. Perhaps it will achieve an additional 999 upvotes and 0 downvotes and be considered better than the latter, though not likely.\n",
        "\n",
        "What we really want is an estimate of the *true upvote ratio*. Note that the true upvote ratio is not the same as the observed upvote ratio: the true upvote ratio is hidden, and we only observe upvotes vs. downvotes (one can think of the true upvote ratio as \"what is the underlying probability someone gives this submission a upvote, versus a downvote\"). So the 999 upvote/1 downvote submission probably has a true upvote ratio close to 1, which we can assert with confidence thanks to the Law of Large Numbers, but on the other hand we are much less certain about the true upvote ratio of the submission with only a single upvote. Sounds like a Bayesian problem to me.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uv4KU6jJZh7"
      },
      "source": [
        "One way to determine a prior on the upvote ratio is to look at the historical distribution of upvote ratios. This can be accomplished by scraping Reddit's submissions and determining a distribution. There are a few problems with this technique though:\n",
        "\n",
        "1. Skewed data:  The vast majority of submissions have very few votes, hence there will be many submissions with ratios near the extremes (see the \"triangular plot\" in the above Kaggle dataset), effectively skewing our distribution to the extremes. One could try to only use submissions with votes greater than some threshold. Again, problems are encountered. There is a tradeoff between number of submissions available to use and a higher threshold with associated ratio precision. \n",
        "2. Biased data: Reddit is composed of different subpages, called subreddits. Two examples are *r/aww*, which posts pics of cute animals, and *r/politics*. It is very likely that the user behaviour towards submissions of these two subreddits are very different: visitors are likely friendly and affectionate in the former, and would therefore upvote submissions more, compared to the latter, where submissions are likely to be controversial and disagreed upon. Therefore not all submissions are the same. \n",
        "\n",
        "\n",
        "In light of these, I think it is better to use a `Uniform` prior.\n",
        "\n",
        "\n",
        "With our prior in place, we can find the posterior of the true upvote ratio. The Python script below will scrape the best posts from the `showerthoughts` community on Reddit. This is a text-only community so the title of each post *is* the post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KaCH4igIUOV"
      },
      "source": [
        "#### Setting up the `Praw` Reddit API\n",
        "\n",
        "Use of the `praw` package for retrieving data from Reddit does require some private information on your Reddit account. As such, we are not releasing the secret keys and reddit account passwords that we originally used for the code cell below. Fortunately, we've provided detailed information on how to set up the next code cell with your custom information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVKFEZmkJZh7"
      },
      "source": [
        "#### Register your Application on Reddit\n",
        "\n",
        "1. Log into your Reddit account.\n",
        "\n",
        "2. Click the down arrow to the right of your name, then click the Preferences button.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*YMLEuY0AXaSVW2YJAbUiaQ.png\" width=\"250\">\n",
        "\n",
        "3. Click the app tab.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*TDDvEVTSWTERxUw2ZrJDWA.png\" width=\"600\">\n",
        "\n",
        "4. Click the create another app button at the bottom left of your screen.\n",
        "\n",
        "5. Populate your script with the required fields. Refer to the screen shot below:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*duC42-xMothcka2WXLKGiw.png\" width=\"600\">\n",
        "\n",
        "6. Hit the create app button once you have populated all fields. You should now have a script which resembles the following:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*v_et9Ei38h0zZ0SdMCV0bQ.png\" width=\"600\">\n",
        "\n",
        "\n",
        "NOTE: Certain components of the `reddit = praw.Reddit(\"BasyesianMethodsForHackers\")` code have been intentionally omitted. This is because praw requires a user ID for accessing Reddit. the praw function follows the following format:\n",
        "```python\n",
        "reddit = praw.Reddit(client_id = 'PERSONAL_USE_SCRIPT_14_CHARS',\n",
        "                     client_secret = 'SECRET_KEY_27_CHARS ',\n",
        "                     user_agent = 'YOUR_APP_NAME',\n",
        "                     username = 'YOUR_REDDIT_USER_NAME',\n",
        "                     password = 'YOUR_REDDIT_LOGIN_PASSWORD')\n",
        "```\n",
        "For help with creating a Reddit instance, visit\n",
        "https://praw.readthedocs.io/en/latest/code_overview/reddit_instance.html.\n",
        "\n",
        "For help on configuring PRAW, visit\n",
        "https://praw.readthedocs.io/en/latest/getting_started/configuration.html."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScHOHPunJZh7"
      },
      "source": [
        "# Reddit API setup\n",
        "from IPython.core.display import Image\n",
        "import praw\n",
        "\n",
        "\n",
        "enter_client_id = 'ZhGqHeR1zTM9fg'\n",
        "enter_client_secret = 'keZdvIa1Ge257NKEm3v-eGEdv8M'\n",
        "enter_user_agent = 'bayesian_app'\n",
        "enter_username = 'ThisIsJustADemo'\n",
        "enter_password = 'EnterYourOwnInfoHere'\n",
        "\n",
        "# e.g. \"showerthoughts\", \"todayilearned\", \"worldnews\", \"science\", \"lifeprotips\", \"nottheonion\"\n",
        "subreddit_name = 'showerthoughts'\n",
        "\n",
        "reddit = praw.Reddit(client_id = enter_client_id,\n",
        "                     client_secret = enter_client_secret,\n",
        "                     user_agent = enter_user_agent,\n",
        "                     username = enter_username,\n",
        "                     password = enter_password)\n",
        "subreddit  = reddit.subreddit(subreddit_name)\n",
        "\n",
        "# Go by timespan - 'hour', 'day', 'week', 'month', 'year', 'all'\n",
        "# might need to go longer than an hour to get entries...\n",
        "\n",
        "# e.g. 'hour', 'day', 'week', 'month', 'year', 'all'\n",
        "timespan = 'day'\n",
        "\n",
        "top_submissions = subreddit.top(timespan)\n",
        "\n",
        "# Adding a number to the inside of int() call will get the ith top post.\n",
        "ith_top_post = 2\n",
        "n_sub = int(ith_top_post)\n",
        "\n",
        "i = 0\n",
        "while i < n_sub:\n",
        "    top_submission = next(top_submissions)\n",
        "    i += 1\n",
        "\n",
        "top_post = top_submission.title\n",
        "\n",
        "upvotes = []\n",
        "downvotes = []\n",
        "contents = []\n",
        "\n",
        "for sub in top_submissions:\n",
        "    try:\n",
        "        ratio = sub.upvote_ratio\n",
        "        ups = int(round((ratio * sub.score) / (2 * ratio - 1))\n",
        "                  if ratio != .5 else round(sub.score / 2))\n",
        "        upvotes.append(ups)\n",
        "        downvotes.append(ups - sub.score)\n",
        "        contents.append(sub.title)\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "votes = np.array([upvotes, downvotes]).T\n",
        "\n",
        "print(\"Post contents: \\n\")\n",
        "print(top_post)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vvhJ7o3JZh-"
      },
      "source": [
        "Above is the top post as well as some other sample posts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v8LKHnX4-z5"
      },
      "source": [
        "# Contents: an array of the text from the last 100 top submissions to a subreddit\n",
        "# votes: a 2d numpy array of upvotes, downvotes for each submission.\n",
        "n_submissions = len(votes)\n",
        "submissions = Uniform(low = 0., high = float(n_submissions)).sample(sample_shape = (4,))\n",
        "submissions = submissions.astype(np.i32)\n",
        "\n",
        "print(f\"Some Submissions (out of {n_submissions} total)\\n-----------\")\n",
        "for i in submissions:\n",
        "    print('\"' + contents[i] + '\"')\n",
        "    print(\"upvotes/downvotes:\", votes[i, :], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwRZoAdAJZiA"
      },
      "source": [
        "For a given true upvote ratio $p$ and $N$ votes, the number of upvotes will look like a Binomial random variable with parameters $p$ and $N$. (This is because of the equivalence between upvote ratio and probability of upvoting versus downvoting, out of $N$ possible votes/trials). We create a function that performs Bayesian inference on $p$, for a particular submission's upvote/downvote pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVsMkqyz0ULQ"
      },
      "source": [
        "def joint_log_prob(upvotes, N, test_upvote_ratio):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      upvotes: observed upvotes for a submission\n",
        "      N : observed upvotes+downvotes for the submission\n",
        "      test_upvote_ratio: hypothesized value for true value of upvote ratio\n",
        "    Returns: \n",
        "      Joint log probability optimization function to compute true upvote ratio.\n",
        "    \"\"\"\n",
        "    # Use a uniform prior\n",
        "    rv_upvote_ratio = Uniform(low = 0., high = 1.)\n",
        "    rv_observations = Binomial(total_count = float(N),\n",
        "                               probs = test_upvote_ratio)\n",
        "    return (\n",
        "          rv_upvote_ratio.log_prob(test_upvote_ratio)\n",
        "        + rv_observations.log_prob(float(upvotes)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Otcm3a31oo"
      },
      "source": [
        "in some cases we might want to run someting like an HMC for multiple, or a variable number, of inputs. Loops are common examples of this. Here we define our function for setting up an HMC that can take in different numbers of upvotes and/or downvotes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aUOU-FLJZiB"
      },
      "source": [
        "def posterior_upvote_ratio(upvotes, downvotes):\n",
        "    num_results = 5000\n",
        "    num_burnin_steps = 1000\n",
        "    N = float(upvotes) + float(downvotes)\n",
        "\n",
        "    # Initialise the step_size (it will be automatically adapted.)\n",
        "    step_size = tf.Variable(\n",
        "        initial_value = tf.constant(.5, dtype = f32),\n",
        "        trainable = False,\n",
        "        name = 'step_size') \n",
        "\n",
        "    # Set the chain's start state.\n",
        "    initial_chain_state = [\n",
        "        .5 * np.ones((), dtype = f32)]\n",
        "\n",
        "    # Since HMC operates over unconstrained space, we need to transform the\n",
        "    # samples so they live in real-space.\n",
        "    unconstraining_bijectors = [\n",
        "        tfb.Sigmoid()          ]\n",
        "\n",
        "    # Define a closure over our joint_log_prob.\n",
        "    unnormalised_posterior_log_prob = lambda *args: joint_log_prob(upvotes, N, *args)\n",
        "\n",
        "    kernel = tfp.mcmc.TransformedTransitionKernel(\n",
        "        inner_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
        "            target_log_prob_fn = unnormalised_posterior_log_prob,\n",
        "            num_leapfrog_steps = 2,\n",
        "            step_size = step_size,\n",
        "            state_gradients_are_stopped = True),\n",
        "        bijector = unconstraining_bijectors)\n",
        "\n",
        "    kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
        "      inner_kernel = kernel,\n",
        "      num_adaptation_steps = int(num_burnin_steps * .8))\n",
        "\n",
        "    # Sample from the chain\n",
        "    # TODO Rename 'posterior_upvote_ratio' since it's duplicate to the fn's name.\n",
        "    (posterior_upvote_ratio,), kernel_results = tfp.mcmc.sample_chain(\n",
        "        num_results = num_results,\n",
        "        num_burnin_steps = num_burnin_steps,\n",
        "        current_state = initial_chain_state,\n",
        "        kernel = kernel)\n",
        "    \n",
        "    return posterior_upvote_ratio, kernel_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLXqEPVj5PoJ"
      },
      "source": [
        "plt.figure(figsize(11., 8))\n",
        "posteriors = []\n",
        "colours = ['#5DA5DA', '#F15854', '#B276B2', '#60BD68', '#F17CB0']\n",
        "\n",
        "for i in range(len(submissions)):\n",
        "    j = submissions[i]\n",
        "    posteriors.append(posterior_upvote_ratio(votes[j, 0], votes[j, 1])[0])\n",
        "    plt.hist(posteriors[i], bins=10, alpha=.9, \n",
        "             histtype='step', color=colours[i], lw=3,\n",
        "             label=f'({} up:{} down)\\n{}...' % (votes[j, 0], votes[j, 1], contents[j][:50]))\n",
        "    plt.hist(posteriors[i], bins=10, alpha=.2, \n",
        "             histtype='stepfilled', color=colours[i], lw=3)\n",
        "    \n",
        "plt.legend(loc='upper left')\n",
        "plt.xlim(0, 1)\n",
        "plt.title(\"Posterior distributions of upvote ratios on different submissions\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyvNr1XUJZiG"
      },
      "source": [
        "Some distributions are very tight, others have very long tails (relatively speaking), expressing our uncertainty with what the true upvote ratio might be.\n",
        "\n",
        "#### Sorting!\n",
        "\n",
        "We have been ignoring the goal of this exercise: how do we sort the submissions from *best to worst*? Of course, we cannot sort distributions, we must sort scalar numbers. There are many ways to distill a distribution down to a scalar: expressing the distribution through its expected value, or mean, is one way. Choosing the mean is a bad choice though. This is because the mean does not take into account the uncertainty of distributions.\n",
        "\n",
        "I  suggest using the *95% least plausible value*, defined as the value such that there is only a 5% chance the true parameter is lower (think of the lower bound on the 95% credible region). Below are the posterior distributions with the 95% least-plausible value plotted:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19LfJYnb2tNk"
      },
      "source": [
        "N = posteriors[0].shape[0]\n",
        "lower_limits = []\n",
        "for i in range(len(submissions)):\n",
        "    j = submissions[i]\n",
        "    plt.hist(posteriors[i], bins=20, alpha=.9, \n",
        "             histtype='step', color=colours[i], lw=3,\n",
        "             label=f'({} up:{} down)\\n{}...' % (votes[j, 0], votes[j, 1], contents[j][:50]))\n",
        "    plt.hist(posteriors[i], bins=20, alpha=.2, \n",
        "             histtype='stepfilled', color=colours[i], lw=3)\n",
        "    v = np.sort(posteriors[i] )[ int(.05 * N)]\n",
        "    plt.vlines(v, 0, 30, color=colours[i], linestyles='--', lw=3)\n",
        "    lower_limits.append(v)\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "plt.title(\"Posterior distributions of upvote ratios on different submissions\");\n",
        "order = np.argsort(-np.array(lower_limits))\n",
        "print(order, lower_limits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGPproRzJZiI"
      },
      "source": [
        "The best submissions, according to our procedure, are the submissions that are *most-likely* to score a high percentage of upvotes. Visually those are the submissions with the 95% least plausible value close to 1.\n",
        "\n",
        "Why is sorting based on this quantity a good idea? By ordering by the 95% least plausible value, we are being the most conservative with what we think is best.  When using the lower-bound of the 95% credible interval, we believe with high certainty that the 'true upvote ratio' is at the very least equal to this value (or greater), thereby ensuring that the best submissions are still on top. Under this ordering, we impose the following very natural properties:\n",
        "\n",
        "1. given two submissions with the same observed upvote ratio, we will assign the submission with more votes as better (since we are more confident it has a higher ratio).\n",
        "2. given two submissions with the same number of votes, we still assign the submission with more upvotes as *better*.\n",
        "\n",
        "#### But this is too slow for real-time!\n",
        "\n",
        "I agree, computing the posterior of every submission takes a long time, and by the time you have computed it, likely the data has changed. I delay the mathematics to the appendix, but I suggest using the following formula to compute the lower bound very fast.\n",
        "\n",
        "$$ \\frac{a}{a + b} - 1.65\\sqrt{ \\frac{ab}{ (a+b)^2(a + b +1 ) } }$$\n",
        "\n",
        "where \n",
        "$$\n",
        "\\begin{align}\n",
        "& a = 1 + u \\\\\n",
        "& b = 1 + d \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "$u$ is the number of upvotes, and $d$ is the number of downvotes. The formula is a shortcut in Bayesian inference, which will be further explained in Chapter 6 when we discuss priors in more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhBY5JmNJZiI"
      },
      "source": [
        "def intervals(u, d):\n",
        "    a = tf.add(1., u)\n",
        "    b = tf.add(1., d)\n",
        "    mu = tf.divide(x = a, y = tf.add(1., u))\n",
        "    std_err = 1.65 * np.sqrt((a * b) / ((a + b) ** 2 * (a + b + 1.)))\n",
        "    \n",
        "    return mu, std_err\n",
        "  \n",
        "print(\"Approximate lower bounds:\")\n",
        "posterior_mean, std_err  = intervals(votes[:, 0], votes[:, 1])\n",
        "lb = posterior_mean - std_err\n",
        "print(lb)\n",
        "print(\"\\n\")\n",
        "print(\"Top 40 Sorted according to approximate lower bounds:\")\n",
        "print(\"\\n\")\n",
        "order = tf.nn.top_k(lb, k=lb.shape[0], sorted=True)\n",
        "ordered_contents = []\n",
        "for i, N in enumerate(order.values[:40]):\n",
        "    ordered_contents.append(contents[i])\n",
        "    print(votes[i, 0], votes[i, 1], contents[i])\n",
        "    print(\"-------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1__TyMJJZiK"
      },
      "source": [
        "We can view the ordering visually by plotting the posterior mean and bounds, and sorting by the lower bound. In the plot below, notice that the left error-bar is sorted (as we suggested this is the best way to determine an ordering), so the means, indicated by dots, do not follow any strong pattern. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHtWEE7_EbJL"
      },
      "source": [
        "r_order = order.indices[::-1][-40:]\n",
        "ratio_range = np.range(len(r_order) - 1, -1, -1) \n",
        "r_order_vals = order.values[::-1][-40:]\n",
        "plt.errorbar(r_order_vals, \n",
        "             np.arange(len(r_order)), \n",
        "             xerr=tf.gather(std_err, r_order), capsize=0, fmt='o',\n",
        "             color=TFColor[0])\n",
        "plt.xlim(.3, 1)\n",
        "plt.yticks(ratio_range, map(lambda x: x[:30].replace('\\n', ''), ordered_contents));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G77bofA0JZiM"
      },
      "source": [
        "In the graphic above, you can see why sorting by mean would be sub-optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx19EikOJZiM"
      },
      "source": [
        "### Extension to Starred rating systems\n",
        "\n",
        "The above procedure works well for upvote-downvotes schemes, but what about systems that use star ratings, e.g. 5 star rating systems. Similar problems apply with simply taking the average: an item with two perfect ratings would beat an item with thousands of perfect ratings, but a single sub-perfect rating. \n",
        "\n",
        "\n",
        "We can consider the upvote-downvote problem above as binary: 0 is a downvote, 1 if an upvote. A $N$-star rating system can be seen as a more continuous version of above, and we can set $n$ stars rewarded is equivalent to rewarding $\\frac{n}{N}$. For example, in a 5-star system, a 2 star rating corresponds to 0.4. A perfect rating is a 1. We can use the same formula as before, but with $a,b$ defined differently:\n",
        "\n",
        "\n",
        "$$ \\frac{a}{a + b} - 1.65\\sqrt{ \\frac{ab}{ (a+b)^2(a + b +1 ) } }$$\n",
        "\n",
        "where \n",
        "$$\n",
        "\\begin{align}\n",
        "& a = 1 + S \\\\\n",
        "& b = 1 + N - S \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "where $N$ is the number of users who rated, and $S$ is the sum of all the ratings, under the equivalence scheme mentioned above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9KpvDCzJZiN"
      },
      "source": [
        "### Example: Counting Github stars\n",
        "\n",
        "What is the average number of stars a Github repository has? How would you calculate this? There are over 6 million respositories, so there is more than enough data to invoke the Law of Large numbers. Let's start pulling some data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AbPT5Ajju3t"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter3_MCMC/data/github_data.csv'\n",
        "filename = wget.download(url)\n",
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8qsQwdgTwg0"
      },
      "source": [
        "# Github data scrapper\n",
        "# See documentation_url: https://developer.github.com/v3/\n",
        "from requests import get\n",
        "\n",
        "\"\"\"\n",
        "variables of interest:\n",
        "    indp. variables\n",
        "    - language, given as a binary variable. Need 4 positions for 5 langagues\n",
        "    - #number of days created ago, 1 position\n",
        "    - has wiki? Boolean, 1 position\n",
        "    - followers, 1 position\n",
        "    - following, 1 position\n",
        "    - constant\n",
        "    \n",
        "    dep. variables\n",
        "    -stars/watchers\n",
        "    -forks\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "MAX = 8000000\n",
        "today =  datetime.datetime.today()\n",
        "randint = np.random.randint\n",
        "N = 20  # Sample size\n",
        "auth = ('mikeshwe', 'kick#Ass1')\n",
        "\n",
        "language_mappings = {'Python': 0, 'JavaScript': 1, 'Ruby': 2, 'Java': 3, 'Shell': 4, 'PHP': 5}\n",
        "\n",
        "# Define data matrix: \n",
        "X = np.zeros((N, 12), dtype = int)\n",
        "\n",
        "for i in range(N):\n",
        "    is_fork = True\n",
        "    is_valid_language = False\n",
        "    \n",
        "    while is_fork or not is_valid_language:\n",
        "        is_fork = True\n",
        "        is_valid_language = False\n",
        "        \n",
        "        params = {'since': randint(0, MAX)}\n",
        "        r = get('https://api.github.com/repositories', params=params, auth=auth)\n",
        "        results = json.loads(r.text)[0]\n",
        "        # im only interested in the first one, and if it is not a fork.\n",
        "        # print(results)\n",
        "        is_fork = results['fork']\n",
        "        \n",
        "        r = get(results['url'], auth=auth)\n",
        "        \n",
        "        # Check the language\n",
        "        repo_results = json.loads(r.text)\n",
        "        try: \n",
        "            language_mappings[repo_results['language']]\n",
        "            is_valid_language = True\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # languages \n",
        "    X[i, language_mappings[repo_results['language']]] = 1\n",
        "    \n",
        "    # delta time\n",
        "    X[i, 6] = (today - datetime.datetime.strptime(repo_results['created_at'][:10], '%Y-%m-%d')).days\n",
        "    \n",
        "    # haswiki\n",
        "    X[i, 7] = repo_results['has_wiki']\n",
        "    \n",
        "    # get user information\n",
        "    r = get(results['owner']['url'], auth=auth)\n",
        "    user_results = json.loads(r.text)\n",
        "    X[i, 8] = user_results['following']\n",
        "    X[i, 9] = user_results['followers']\n",
        "    \n",
        "    # get dep. data\n",
        "    X[i, 10] = repo_results['watchers_count']\n",
        "    X[i, 11] = repo_results['forks_count']\n",
        "    print(\" -------------- \")\n",
        "    print(i, ':', results['full_name'], repo_results['language'], repo_results['watchers_count'], repo_results['forks_count']) \n",
        "    print(\" -------------- \") \n",
        "    \n",
        "np.savetxt('github_data.csv', X, delimiter=',', fmt='%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0rE6kLWJZiN"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "While the Law of Large Numbers is cool, it is only true so much as its name implies: with large sample sizes only. We have seen how our inference can be affected by not considering *how the data is shaped*. \n",
        "\n",
        "1. By (cheaply) drawing many samples from the posterior distributions, we can ensure that the Law of Large Number applies as we approximate expected values (which we will do in the next chapter).\n",
        "\n",
        "2. Bayesian inference understands that with small sample sizes, we can observe wild randomness. Our posterior distribution will reflect this by being more spread rather than tightly concentrated. Thus, our inference should be correctable.\n",
        "\n",
        "3. There are major implications of not considering the sample size, and trying to sort objects that are unstable leads to pathological orderings. The method provided above solves this problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YGvCpGyJZiN"
      },
      "source": [
        "### Appendix\n",
        "\n",
        "##### Derivation of sorting submissions formula\n",
        "\n",
        "Basically what we are doing is using a Beta prior (with parameters $a=1, b=1$, which is a uniform distribution), and using a Binomial likelihood with observations $u, N = u+d$. This means our posterior is a Beta distribution with parameters $a' = 1 + u, b' = 1 + (N - u) = 1+d$. We then need to find the value, $x$, such that 0.05 probability is less than $x$. This is usually done by inverting the CDF ([Cumulative Distribution Function](http://en.wikipedia.org/wiki/Cumulative_Distribution_Function)), but the CDF of the beta, for integer parameters, is known but is a large sum [3]. \n",
        "\n",
        "We instead use a Normal approximation. The mean of the Beta is $\\mu = a'/(a'+b')$ and the variance is \n",
        "\n",
        "$$\\sigma^2 = \\frac{a'b'}{ (a' + b')^2(a'+b'+1) }$$\n",
        "\n",
        "Hence we solve the following equation for $x$ and have an approximate lower bound. \n",
        "\n",
        "$$ 0.05 = \\Phi\\left( \\frac{(x - \\mu)}{\\sigma}\\right) $$ \n",
        "\n",
        "$\\Phi$ being the [cumulative distribution for the normal distribution](http://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDZNt3PrJZiO"
      },
      "source": [
        "##### Exercises\n",
        "\n",
        "1\\. How would you estimate the quantity $E\\left[ \\cos{X} \\right]$, where $X \\sim \\text{Exp}(4)$? What about $E\\left[ \\cos{X} | X \\lt 1\\right]$, i.e. the expected value *given* we know $X$ is less than 1? Would you need more samples than the original samples size to be equally accurate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKqgiSdAJZiO"
      },
      "source": [
        "## Enter code here\n",
        "%%time\n",
        "exp = Exponential(rate = 4.)\n",
        "N = 10000\n",
        "X = exp.sample(sample_shape = int(N))\n",
        "print(X)\n",
        "  \n",
        "## ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhwWG-sdJZiQ"
      },
      "source": [
        "2\\. The following table was located in the paper \"Going for Three: Predicting the Likelihood of Field Goal Success with Logistic Regression\" [2]. The table ranks football field-goal kickers by their percent of non-misses. What mistake have the researchers made?\n",
        "\n",
        "-----\n",
        "\n",
        "####  Kicker Careers Ranked by Make Percentage\n",
        "<table><tbody><tr><th>Rank </th><th>Kicker </th><th>Make % </th><th>Number  of Kicks</th></tr><tr><td>1 </td><td>Garrett Hartley </td><td>87.7 </td><td>57</td></tr><tr><td>2</td><td> Matt Stover </td><td>86.8 </td><td>335</td></tr><tr><td>3 </td><td>Robbie Gould </td><td>86.2 </td><td>224</td></tr><tr><td>4 </td><td>Rob Bironas </td><td>86.1 </td><td>223</td></tr><tr><td>5</td><td> Shayne Graham </td><td>85.4 </td><td>254</td></tr><tr><td>… </td><td>… </td><td>…</td><td> </td></tr><tr><td>51</td><td> Dave Rayner </td><td>72.2 </td><td>90</td></tr><tr><td>52</td><td> Nick Novak </td><td>71.9 </td><td>64</td></tr><tr><td>53 </td><td>Tim Seder </td><td>71.0 </td><td>62</td></tr><tr><td>54 </td><td>Jose Cortez </td><td>70.7</td><td> 75</td></tr><tr><td>55 </td><td>Wade Richey </td><td>66.1</td><td> 56</td></tr></tbody></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iQ9mmGkJZiQ"
      },
      "source": [
        "In August 2013, [a popular post](http://bpodgursky.wordpress.com/2013/08/21/average-income-per-programming-language/) on the average income per programmer of different languages was trending. Here's the summary chart: (reproduced without permission, cause when you lie with stats, you gunna get the hammer). What do you notice about the extremes?\n",
        "\n",
        "------\n",
        "\n",
        "#### Average household income by programming language\n",
        "\n",
        "<table >\n",
        " <tr><td>Language</td><td>Average Household Income ($)</td><td>Data Points</td></tr>\n",
        " <tr><td>Puppet</td><td>87,589.29</td><td>112</td></tr>\n",
        " <tr><td>Haskell</td><td>89,973.82</td><td>191</td></tr>\n",
        " <tr><td>PHP</td><td>94,031.19</td><td>978</td></tr>\n",
        " <tr><td>CoffeeScript</td><td>94,890.80</td><td>435</td></tr>\n",
        " <tr><td>VimL</td><td>94,967.11</td><td>532</td></tr>\n",
        " <tr><td>Shell</td><td>96,930.54</td><td>979</td></tr>\n",
        " <tr><td>Lua</td><td>96,930.69</td><td>101</td></tr>\n",
        " <tr><td>Erlang</td><td>97,306.55</td><td>168</td></tr>\n",
        " <tr><td>Clojure</td><td>97,500.00</td><td>269</td></tr>\n",
        " <tr><td>Python</td><td>97,578.87</td><td>2314</td></tr>\n",
        " <tr><td>JavaScript</td><td>97,598.75</td><td>3443</td></tr>\n",
        " <tr><td>Emacs Lisp</td><td>97,774.65</td><td>355</td></tr>\n",
        " <tr><td>C#</td><td>97,823.31</td><td>665</td></tr>\n",
        " <tr><td>Ruby</td><td>98,238.74</td><td>3242</td></tr>\n",
        " <tr><td>C++</td><td>99,147.93</td><td>845</td></tr>\n",
        " <tr><td>CSS</td><td>99,881.40</td><td>527</td></tr>\n",
        " <tr><td>Perl</td><td>100,295.45</td><td>990</td></tr>\n",
        " <tr><td>C</td><td>100,766.51</td><td>2120</td></tr>\n",
        " <tr><td>Go</td><td>101,158.01</td><td>231</td></tr>\n",
        " <tr><td>Scala</td><td>101,460.91</td><td>243</td></tr>\n",
        " <tr><td>ColdFusion</td><td>101,536.70</td><td>109</td></tr>\n",
        " <tr><td>Objective-C</td><td>101,801.60</td><td>562</td></tr>\n",
        " <tr><td>Groovy</td><td>102,650.86</td><td>116</td></tr>\n",
        " <tr><td>Java</td><td>103,179.39</td><td>1402</td></tr>\n",
        " <tr><td>XSLT</td><td>106,199.19</td><td>123</td></tr>\n",
        " <tr><td>ActionScript</td><td>108,119.47</td><td>113</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09kE1C-vJZiQ"
      },
      "source": [
        "### References\n",
        "\n",
        "1. Wainer, Howard. *The Most Dangerous Equation*. American Scientist, Volume 95.\n",
        "2. Clarck, Torin K., Aaron W. Johnson, and Alexander J. Stimpson. \"Going for Three: Predicting the Likelihood of Field Goal Success with Logistic Regression.\" (2013): n. page. [Web](http://www.sloansportsconference.com/wp-content/uploads/2013/Going%20for%20Three%20Predicting%20the%20Likelihood%20of%20Field%20Goal%20Success%20with%20Logistic%20Regression.pdf). 20 Feb. 2013.\n",
        "3. http://en.wikipedia.org/wiki/Beta_function#Incomplete_beta_function"
      ]
    }
  ]
}
